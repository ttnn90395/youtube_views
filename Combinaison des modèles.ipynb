{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 95690,
          "databundleVersionId": 12066300,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31040,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# DataLoader pour mod√®le texte\nval_loader_text = DataLoader(\n    TensorDataset(input_ids_val, y_val),\n    batch_size=64,\n    shuffle=False\n)\n\n# DataLoader pour mod√®le image\nval_loader_img = DataLoader(\n    TensorDataset(val_images, y_val),\n    batch_size=64,\n    shuffle=False\n)\n\n\n# 1) Validation : recherche du meilleur alpha\nmodel_text.eval()\nmodel_img.eval()\n\npreds_val_text, preds_val_img, targets_val = [], [], []\n\nwith torch.no_grad():\n    for (input_ids_batch, y_text_batch), (images_batch, y_img_batch) in zip(val_loader_text, val_loader_img):\n        # Texte\n        input_ids_batch = input_ids_batch.to(device)\n        preds_text = model_text(input_ids_batch)\n        preds_val_text.extend(preds_text.cpu().numpy())\n\n        # Image\n        images_batch = images_batch.to(device)\n        preds_img = model_img(images_batch)\n        preds_val_img.extend(preds_img.cpu().numpy())\n\n        targets_val.extend(y_text_batch.cpu().numpy())  # m√™mes cibles\n\npreds_val_text = np.array(preds_val_text)\npreds_val_img = np.array(preds_val_img)\ntargets_val = np.array(targets_val)\n\n# Recherche de l'alpha optimal\nbest_alpha, best_loss = 0.0, float('inf')\nloss_fn = nn.HuberLoss()\n\nfor alpha in np.linspace(0, 1, 21):\n    combined = alpha * preds_val_text + (1 - alpha) * preds_val_img\n    loss = loss_fn(torch.tensor(combined), torch.tensor(targets_val)).item()\n    if loss < best_loss:\n        best_loss = loss\n        best_alpha = alpha\n\nprint(f\"‚úÖ Meilleur alpha trouv√© : {best_alpha:.2f} | Validation Loss: {best_loss:.4f}\")\n\n# 2)Test : g√©n√©ration des pr√©dictions \n\n# 2.1 Texte\ntest_df[\"text\"] = test_df[\"title\"].fillna(\"\") + \" \" + test_df[\"description\"].fillna(\"\")\ntokenized_test = [text.lower() for text in test_df[\"text\"]]\nX_text_test = torch.tensor([text_to_indices(t, vocab) for t in tokenized_test], dtype=torch.long)\n\nmodel_text.eval()\nwith torch.no_grad():\n    preds_text_test = model_text(X_text_test.to(device)).cpu().numpy()\n\n# 2.2 Images\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor()\n])\n\nclass TestImageDataset(Dataset):\n    def __init__(self, df, data_path, transform=None):\n        self.df = df\n        self.data_path = data_path\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_id = self.df.iloc[idx][\"id\"]\n        path = os.path.join(self.data_path, \"test\", f\"{img_id}.jpg\")\n        image = Image.open(path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        return image\n\nimage_dataset = TestImageDataset(test_df, data_path=data_path, transform=transform)\nimage_loader = DataLoader(image_dataset, batch_size=64)\n\nmodel_img.eval()\npreds_img_test = []\nwith torch.no_grad():\n    for batch in tqdm(image_loader, desc=\"üñº Predicting image model\"):\n        batch = batch.to(device)\n        preds = model_img(batch)\n        preds_img_test.extend(preds.cpu().numpy())\n\npreds_img_test = np.array(preds_img_test)\npreds_text_test = np.array(preds_text_test)\n\n# 3) Moyenne pond√©r√©e & cr√©ation du CSV \navg_preds = best_alpha * preds_text_test + (1 - best_alpha) * preds_img_test\nfinal_views = np.expm1(avg_preds).astype(int)\n\nsubmission = pd.DataFrame({\n    \"ID\": test_df[\"id\"],\n    \"views\": final_views\n})\nsubmission.to_csv(\"/kaggle/working/submission.csv\", index=False)\nprint(f\" submission.csv g√©n√©r√© avec succ√®s avec alpha = {best_alpha:.2f}\")\n",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-29T09:48:50.749043Z",
          "iopub.execute_input": "2025-05-29T09:48:50.749355Z",
          "iopub.status.idle": "2025-05-29T09:49:03.874224Z",
          "shell.execute_reply.started": "2025-05-29T09:48:50.749335Z",
          "shell.execute_reply": "2025-05-29T09:49:03.873527Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "‚úÖ Meilleur alpha trouv√© : 0.70 | Validation Loss: 0.7375\n"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "üñº Predicting image model: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 54/54 [00:12<00:00,  4.35it/s]"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "‚úÖ submission.csv g√©n√©r√© avec succ√®s avec alpha = 0.70\n"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "\n"
        }
      ],
      "execution_count": 29
    },
    {
      "cell_type": "code",
      "source": "from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\n\n# Donn√©es de validation \nval_df = train_df.iloc[idx_val]\nval_texts = val_df[\"title\"].fillna(\"\") + \" \" + val_df[\"description\"].fillna(\"\")\nval_prompts = val_df.apply(lambda row: f\"Video titled '{row['title']}', from channel '{row['channel']}', published in {row['year']}.\", axis=1)\nval_input_ids = torch.tensor([text_to_indices(t, vocab) for t in val_texts], dtype=torch.long).to(device)\n\n# Images val\ndef load_val_images(df_subset):\n    images = []\n    for img_id in df_subset[\"id\"]:\n        path = get_image_path(img_id)\n        image = Image.open(path).convert(\"RGB\")\n        image = transform(image)\n        images.append(image)\n    return torch.stack(images)\n\n# On garde les images PIL pour le mod√®le CLIP, et on transforme pour les autres\nval_images_pil = []\nval_images_tensor = []\n\nfor img_id in val_df[\"id\"]:\n    path = get_image_path(img_id)\n    img = Image.open(path).convert(\"RGB\")\n    val_images_pil.append(img)                            # pour CLIP\n    val_images_tensor.append(transform(img))              # pour CNN\n\nval_images_tensor = torch.stack(val_images_tensor).to(device)\n\nval_targets = y[idx_val]\n\n# Pr√©dictions sur les mod√®les de base \nmodel_text.eval()\nmodel_img.eval()\nmodel_clip.eval()\nmodel_1.eval()\nmodel_2.eval()\nmodel_3.eval()\nmodel_4.eval()\nmodel_5.eval()\n\n# M√©tadonn√©es pour validation\nmeta_val = torch.tensor(meta_features[idx_val], dtype=torch.float32).to(device)\n\n\nwith torch.no_grad():\n    val_preds_text = model_text(val_input_ids).cpu().numpy()\n    val_preds_img = model_img(val_images_tensor).cpu().numpy()\n\n    clip_inputs = model_clip.processor(\n        images=val_images_pil,\n        text=list(val_prompts),\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True\n    ).to(device)\n    output = model_clip.clip(**clip_inputs)\n    clip_feats = torch.cat([output.image_embeds, output.text_embeds], dim=1)\n    val_preds_clip = model_clip.head(clip_feats).squeeze(1).cpu().numpy()\n\n    val_preds_1 = model_1(val_images_tensor, meta_val).cpu().numpy()\n    val_preds_2 = model_2(val_images_tensor, meta_val).cpu().numpy()\n    val_preds_3 = model_3(val_images_tensor, meta_val).cpu().numpy()\n    val_preds_4 = model_4(val_images_tensor, meta_val).cpu().numpy()\n    val_preds_5 = model_5(val_images_tensor, meta_val).cpu().numpy()\n\n\n\n# Apprentissage des pond√©rations\nX_stack = np.vstack([\n    val_preds_text,\n    val_preds_img,\n    val_preds_clip,\n    val_preds_1,\n    val_preds_2,\n    val_preds_3,\n    val_preds_4,\n    val_preds_5\n]).T\n\nreg = Ridge(alpha=1.0)\nreg.fit(X_stack, val_targets)\n\ncoefs = reg.coef_\ncoef_names = [\"text\", \"img\", \"clip\", \"model_1\", \"model_2\", \"model_3\", \"model_4\", \"model_5\"]\n\nprint(\"Pond√©rations apprises :\")\nfor name, val in zip(coef_names, coefs):\n    print(f\"  {name:>8} : {val:.4f}\")\n",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-29T17:17:53.139026Z",
          "iopub.execute_input": "2025-05-29T17:17:53.139242Z",
          "iopub.status.idle": "2025-05-29T17:17:55.965403Z",
          "shell.execute_reply.started": "2025-05-29T17:17:53.139215Z",
          "shell.execute_reply": "2025-05-29T17:17:55.964249Z"
        }
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_35/2120534389.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# --- Donn√©es de validation ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mval_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"description\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mval_prompts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"Video titled '{row['title']}', from channel '{row['channel']}', published in {row['year']}.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
          ],
          "ename": "NameError",
          "evalue": "name 'train_df' is not defined",
          "output_type": "error"
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": "from torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# Calcul du nombre total de features\nmeta_input_dim = text_features.shape[1] + channel_features.shape[1] + 1\n\nX_train, X_val, idx_train, idx_val = train_test_split(\n    np.arange(len(train_df)), np.arange(len(train_df)), test_size=0.1, random_state=42)\n\ntrain_dataset = YouTubeDataset(train_df.iloc[idx_train], text_features[idx_train], channel_features[idx_train], year_features[idx_train], y[idx_train], data_path)\nval_dataset = YouTubeDataset(train_df.iloc[idx_val], text_features[idx_val], channel_features[idx_val], year_features[idx_val], y[idx_val], data_path)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64)\n\nmeta_input_dim=5047\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_1 = YouTubeModel(meta_input_dim=meta_input_dim).to(device)\nmodel_2 = YouTubeModel_upgrade1(meta_input_dim=meta_input_dim).to(device)\nmodel_3 = YouTubeModel_LiteAttention(meta_input_dim=meta_input_dim).to(device)\nmodel_4 = YouTubeModelSimple(meta_input_dim=meta_input_dim).to(device)\nmodel_5 = YouTubeModel_CNN_MetaAttention(meta_input_dim=meta_input_dim).to(device)\n\nmodel = model_5\n\ndef entraine(premier_model):\n    optimizer = torch.optim.Adam(premier_model.parameters(), lr=1e-3)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n    loss_fn = nn.HuberLoss()\n\n    from tqdm.notebook import tqdm  \n\n    for epoch in range(7):\n        premier_model.train()\n        train_losses = []\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} [train]\")\n        for images, metas, targets in pbar:\n            images, metas, targets = images.to(device), metas.to(device), targets.to(device)\n            preds = premier_model(images, metas)\n            loss = loss_fn(preds, targets)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_losses.append(loss.item())\n            pbar.set_postfix({\"loss\": np.mean(train_losses)})\n\n        # Validation\n        premier_model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for images, metas, targets in val_loader:\n                images, metas, targets = images.to(device), metas.to(device), targets.to(device)\n                preds = premier_model(images, metas)\n                val_losses.append(loss_fn(preds, targets).item())\n        scheduler.step(np.mean(val_losses))\n        print(f\" Epoch {epoch+1} done | Train Loss: {np.mean(train_losses):.4f} | Val Loss: {np.mean(val_losses):.4f}\")\n\nfor model in [model_1, model_2, model_3, model_4, model_5]:\n    entraine(model)\n",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-29T15:23:22.846581Z",
          "iopub.execute_input": "2025-05-29T15:23:22.846858Z",
          "iopub.status.idle": "2025-05-29T15:23:23.216782Z",
          "shell.execute_reply.started": "2025-05-29T15:23:22.846839Z",
          "shell.execute_reply": "2025-05-29T15:23:23.215578Z"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Epoch 1 [train]:   0%|          | 0/436 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d4c3ddb34f9439a80af0724c7e81fbb"
            }
          },
          "metadata": {}
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_35/1919694721.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mentraine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipykernel_35/1919694721.py\u001b[0m in \u001b[0;36mentraine\u001b[0;34m(premier_model)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpremier_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_35/2920730073.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image, meta)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mimg_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2908\u001b[0m             \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2909\u001b[0m         )\n\u001b[0;32m-> 2910\u001b[0;31m     return torch.layer_norm(\n\u001b[0m\u001b[1;32m   2911\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2912\u001b[0m     )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[5047], expected input with shape [*, 5047], but got input of size[32, 559]"
          ],
          "ename": "RuntimeError",
          "evalue": "Given normalized_shape=[5047], expected input with shape [*, 5047], but got input of size[32, 559]",
          "output_type": "error"
        }
      ],
      "execution_count": 86
    },
    {
      "cell_type": "code",
      "source": "from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\n\n#Donn√©es de validation\nval_df = train_df.iloc[idx_val]\nval_texts = val_df[\"title\"].fillna(\"\") + \" \" + val_df[\"description\"].fillna(\"\")\nval_prompts = val_df.apply(lambda row: f\"Video titled '{row['title']}', from channel '{row['channel']}', published in {row['year']}.\", axis=1)\nval_input_ids = torch.tensor([text_to_indices(t, vocab) for t in val_texts], dtype=torch.long).to(device)\n\n# Images val\nval_images_pil = []\nval_images_tensor = []\n\nfor img_id in val_df[\"id\"]:\n    path = get_image_path(img_id)\n    img = Image.open(path).convert(\"RGB\")\n    val_images_pil.append(img)                            # pour CLIP\n    val_images_tensor.append(transform(img))              # pour CNN\n\nval_images_tensor = torch.stack(val_images_tensor).to(device)\nmeta_val = torch.tensor(meta_features[idx_val], dtype=torch.float32).to(device)\nval_targets = y[idx_val]\n\ndef predict_in_batches(model, images_tensor, meta_tensor=None, batch_size=64):\n    model.eval()\n    preds = []\n    with torch.no_grad():\n        for i in range(0, len(images_tensor), batch_size):\n            imgs = images_tensor[i:i+batch_size]\n            if meta_tensor is not None:\n                metas = meta_tensor[i:i+batch_size]\n                out = model(imgs, metas)\n            else:\n                out = model(imgs)\n            preds.extend(out.cpu().numpy())\n    return np.array(preds)\n\n#Mod√®les en √©val\nmodel_text.eval()\nmodel_img.eval()\nmodel_clip.eval()\nmodel_1.eval()\nmodel_2.eval()\nmodel_3.eval()\nmodel_4.eval()\nmodel_5.eval()\n\n#Pr√©dictions\nwith torch.no_grad():\n    # Texte\n    val_preds_text = model_text(val_input_ids).cpu().numpy()\n\n    # Image CNN\n    val_preds_img = predict_in_batches(model_img, val_images_tensor)\n\n    # CLIP\n    clip_inputs = model_clip.processor(\n        images=val_images_pil,\n        text=list(val_prompts),\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True\n    ).to(device)\n    output = model_clip.clip(**clip_inputs)\n    clip_feats = torch.cat([output.image_embeds, output.text_embeds], dim=1)\n    val_preds_clip = model_clip.head(clip_feats).squeeze(1).cpu().numpy()\n\n    # Mod√®les fusionn√©s image + meta\n    val_preds_1 = predict_in_batches(model_1, val_images_tensor, meta_val)\n    val_preds_2 = predict_in_batches(model_2, val_images_tensor, meta_val)\n    val_preds_3 = predict_in_batches(model_3, val_images_tensor, meta_val)\n    val_preds_4 = predict_in_batches(model_4, val_images_tensor, meta_val)\n    val_preds_5 = predict_in_batches(model_5, val_images_tensor, meta_val)\n\n# Apprentissage des pond√©rations\nX_stack = np.vstack([\n    val_preds_text,\n    val_preds_img,\n    val_preds_clip,\n    val_preds_1,\n    val_preds_2,\n    val_preds_3,\n    val_preds_4,\n    val_preds_5\n]).T\n\nreg = Ridge(alpha=1.0)\nreg.fit(X_stack, val_targets)\n\ncoefs = reg.coef_\ncoef_names = [\"text\", \"img\", \"clip\", \"model_1\", \"model_2\", \"model_3\", \"model_4\", \"model_5\"]\n\nprint(\" Pond√©rations apprises :\")\nfor name, val in zip(coef_names, coefs):\n    print(f\"  {name:>8} : {val:.4f}\")\n",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-29T15:03:27.368183Z",
          "iopub.execute_input": "2025-05-29T15:03:27.368480Z",
          "iopub.status.idle": "2025-05-29T15:03:53.094060Z",
          "shell.execute_reply.started": "2025-05-29T15:03:27.368458Z",
          "shell.execute_reply": "2025-05-29T15:03:53.093301Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "‚úÖ Pond√©rations apprises :\n      text : 0.4020\n       img : 0.1964\n      clip : 0.6108\n   model_1 : 0.2133\n   model_2 : -0.1588\n   model_3 : -0.9006\n   model_4 : -0.1647\n   model_5 : -0.1082\n"
        }
      ],
      "execution_count": 81
    },
    {
      "cell_type": "code",
      "source": "import os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom torchvision import transforms\nimport numpy as np\nfrom tqdm import tqdm\nimport pandas as pd\n\n# Texte brut et prompt CLIP\ntest_df[\"text\"] = test_df[\"title\"].fillna(\"\") + \" \" + test_df[\"description\"].fillna(\"\")\ntest_df[\"prompt\"] = test_df.apply(lambda row: f\"Video titled '{row['title']}', from channel '{row['channel']}', published in {row['year']}.\", axis=1)\n\n# Transformation image\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor()\n])\n\n# Dataset unifi√©\nclass UnifiedTestDataset(Dataset):\n    def __init__(self, df, vocab, meta_feats, transform=None):\n        self.df = df\n        self.vocab = vocab\n        self.meta = meta_feats\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_id = row[\"id\"]\n        image_path = os.path.join(data_path, \"test\", f\"{img_id}.jpg\")\n        image = Image.open(image_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n\n        text = row[\"text\"].lower()\n        input_ids = text_to_indices(text, self.vocab)\n        prompt = row[\"prompt\"]\n        meta = self.meta[idx]\n\n        return image, torch.tensor(input_ids, dtype=torch.long), prompt, torch.tensor(meta, dtype=torch.float32), row[\"id\"]\n\n#Collate function\ndef collate_fn(batch):\n    images, input_ids, prompts, metas, ids = zip(*batch)\n    return (\n        torch.stack(images),\n        torch.stack(input_ids),\n        list(prompts),\n        torch.stack(metas),\n        list(ids)\n    )\n\n# DataLoader\ntest_dataset = UnifiedTestDataset(test_df, vocab, meta_features_test, transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=64, collate_fn=collate_fn)\n\n# Mod√®les en √©val\nmodels = [model_text, model_img, model_clip, model_1, model_2, model_3, model_4, model_5]\nfor m in models:\n    m.eval()\n\nclip_processor = model_clip.processor\ncoefs = np.array(coefs)  # les coefficients appris pr√©c√©demment\ncoefs = coefs / coefs.sum()  # normalisation\n\n#Pr√©diction en batch\npreds_all = [[] for _ in range(len(coefs))]\nall_ids = []\n\nwith torch.no_grad():\n    for images, input_ids, prompts, metas, ids in tqdm(test_loader, desc=\"üîÆ Test predictions\"):\n        images = images.to(device)\n        input_ids = input_ids.to(device)\n        metas = metas.to(device)\n\n        # Texte\n        pred_text = model_text(input_ids).cpu().numpy()\n        preds_all[0].extend(pred_text)\n\n        # Image seule\n        pred_img = model_img(images).cpu().numpy()\n        preds_all[1].extend(pred_img)\n\n        # CLIP\n        clip_inputs = clip_processor(images=images, text=prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n        output = model_clip.clip(**clip_inputs)\n        clip_feats = torch.cat([output.image_embeds, output.text_embeds], dim=1)\n        pred_clip = model_clip.head(clip_feats).squeeze(1).cpu().numpy()\n        preds_all[2].extend(pred_clip)\n\n        # Mod√®les 1 √† 5\n                # Mod√®les 1 √† 5\n        for i, model in enumerate(models[3:], start=3):\n            # Pr√©parer les features individuellement\n            pred = model(\n                images,\n                metas  # Assur√© d'√™tre [batch_size, 5047] comme attendu\n            ).cpu().numpy()\n            preds_all[i].extend(pred)\n\n\n        all_ids.extend(ids)\n\n# Moyenne pond√©r√©e finale\nfinal_preds = sum(c * np.array(p) for c, p in zip(coefs, preds_all))\nfinal_views = np.expm1(final_preds).astype(int)\n\n# 9. Soumission\nsubmission = pd.DataFrame({\n    \"ID\": all_ids,\n    \"views\": final_views\n})\nsubmission.to_csv(\"/kaggle/working/submission.csv\", index=False)\nprint(\"Fichier submission.csv g√©n√©r√© avec succ√®s avec les 8 mod√®les pond√©r√©s.\")\n",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-29T15:10:49.747566Z",
          "iopub.execute_input": "2025-05-29T15:10:49.747872Z",
          "iopub.status.idle": "2025-05-29T15:10:50.821854Z",
          "shell.execute_reply.started": "2025-05-29T15:10:49.747851Z",
          "shell.execute_reply": "2025-05-29T15:10:50.820484Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "üîÆ Test predictions:   0%|          | 0/54 [00:00<?, ?it/s]\n"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_35/2953534328.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m# Pr√©parer les features individuellement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             pred = model(\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mmetas\u001b[0m  \u001b[0;31m# Assur√© d'√™tre [batch_size, 5047] comme attendu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_35/2920730073.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image, meta)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mimg_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2908\u001b[0m             \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2909\u001b[0m         )\n\u001b[0;32m-> 2910\u001b[0;31m     return torch.layer_norm(\n\u001b[0m\u001b[1;32m   2911\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2912\u001b[0m     )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[559], expected input with shape [*, 559], but got input of size[64, 5047]"
          ],
          "ename": "RuntimeError",
          "evalue": "Given normalized_shape=[559], expected input with shape [*, 559], but got input of size[64, 5047]",
          "output_type": "error"
        }
      ],
      "execution_count": 84
    }
  ]
}
